#!/usr/bin/env python3
"""
Prepare Website Data - Content Processing Pipeline

This module processes generated markdown files and CSV data to create
a structured JSON/TypeScript file for the Next.js website. It serves
as the bridge between the Python backend discovery system and the
React frontend.

Key Features:
- Parses daily tools digest from markdown format
- Extracts news articles from aggregated news files
- Processes master tools directory from CSV
- Generates TypeScript content file for website
- Handles data validation and error recovery
- Maintains chronological ordering of content

The script implements a robust parsing system that can handle
various markdown formats and gracefully recover from missing
or malformed data files.

Author: AI Insights Daily Team
Version: 3.1.0
Last Updated: June 2025
"""

import os
import json
import csv
import re
from datetime import datetime

# =============================================================================
# CONFIGURATION CONSTANTS
# =============================================================================

# File paths for input and output
ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
ARTIFACTS_DIR = os.path.join(ROOT_DIR, "artifacts")  # Generated content directory
DAILY_TOOLS_MD = os.path.join(ARTIFACTS_DIR, "ai-tools-daily.md")  # Daily tools digest
NEWS_MD = os.path.join(ARTIFACTS_DIR, "blogs-and-news.md")  # News aggregation
MASTER_CSV = os.path.join(ROOT_DIR, "data/master_resources.csv")  # Master tools database
OUTPUT_TS = os.path.join(ROOT_DIR, "website/src/data/content.ts")  # Website content output

# =============================================================================
# PARSING FUNCTIONS
# =============================================================================

def parse_daily_tools(content):
    """
    Parse the ai-tools-daily.md file to extract daily tool discoveries.
    
    This function uses regex patterns to extract structured data from
    the markdown format generated by the daily tools digest script.
    It handles the specific format with date headers and numbered lists.
    
    Args:
        content (str): Raw markdown content from ai-tools-daily.md
        
    Returns:
        list: List of daily tool updates with structured data
    """
    # Split content by date sections using regex
    # Pattern matches: "## YYYY-MM-DD - Daily AI Tools Digest"
    sections = re.split(r'## (\d{4}-\d{2}-\d{2}) - Daily AI Tools Digest', content)
    if not sections:
        return []

    daily_updates = []
    
    # Process sections in pairs (date + content)
    # Skip the first empty section that occurs before the first date
    for i in range(1, len(sections), 2):
        date_str = sections[i].strip()
        
        # Parse date string to ISO format for consistent handling
        try:
            date = datetime.strptime(date_str, '%Y-%m-%d').isoformat()
        except ValueError:
            # Skip invalid date formats
            continue

        tools_content = sections[i+1]
        
        # Extract tool information using regex pattern
        # Pattern matches: "1. Tool Name – [URL](URL) – Description"
        tool_items = re.findall(r'\d+\.\s(.*?)\s–\s\[(.*?)\]\((.*?)\)\s–\s(.*?)\n', tools_content)

        tools = []
        for item in tool_items:
            tools.append({
                "name": item[0].strip(),
                "url": item[2].strip(),
                "description": item[3].strip()
            })
        
        # Only add updates that contain tools
        if tools:
            daily_updates.append({
                "date": date,
                "type": "tools",
                "title": f"AI Tools of the Day: {date_str}",
                "data": tools
            })

    return daily_updates

def parse_news(content):
    """
    Parse the blogs-and-news.md file to extract daily news articles.
    
    This function extracts news articles from the aggregated news markdown
    file. It handles the specific format with date sections and numbered
    lists, including source attribution and links.
    
    Args:
        content (str): Raw markdown content from blogs-and-news.md
        
    Returns:
        list: List of daily news updates with structured data
    """
    daily_updates = []
    
    # Find all date sections using regex
    # Pattern matches: "## Quick Daily AI News [Date]" followed by content
    date_sections = re.findall(r'## Quick Daily AI News (.*?)\n(.*?)(?=\n##|\Z)', content, re.S)
    
    for date_str, section_content in date_sections:
        # Parse date string to ISO format
        try:
            date = datetime.strptime(date_str.strip(), '%B %d, %Y').isoformat()
        except ValueError:
            # Skip invalid date formats
            continue

        # Extract news items from numbered lists
        # Pattern matches: "1. Title - Source [1]"
        news_items = re.findall(r'\d+\.\s(.*?)\s-\s(.*?)\s\[\d+\]', section_content)
        
        # Find source links section at the bottom of each date section
        sources_block = re.search(r'Sources:\n(.*?)(?=\n---|\Z)', section_content, re.S)
        if not sources_block:
            continue
        
        # Extract all source URLs
        links = re.findall(r'\[\d+\]\s(https.*?)\s', sources_block.group(1))

        articles = []
        # Combine titles and links, assuming they are in the same order
        for i, (title, source) in enumerate(news_items):
            if i < len(links):
                articles.append({
                    "title": title.strip(),
                    "url": links[i].strip(),
                    "source": source.strip()
                })
        
        # Only add updates that contain articles
        if articles:
            daily_updates.append({
                "date": date,
                "type": "news",
                "title": f"AI News for {date_str.strip()}",
                "data": articles
            })
            
    return daily_updates

def parse_master_tools():
    """
    Parse the master_resources.csv file to extract all discovered tools.
    
    This function reads the master CSV file containing all discovered
    AI tools and organizes them by category for the tools directory
    section of the website.
    
    Returns:
        dict: Tools organized by category with structured data
    """
    if not os.path.exists(MASTER_CSV):
        return {}
    
    tools_by_category = {}
    
    try:
        with open(MASTER_CSV, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                # Get category with fallback to "Other"
                category = row.get("Category", "Other").strip()
                
                # Initialize category list if it doesn't exist
                if category not in tools_by_category:
                    tools_by_category[category] = []
                
                # Add tool to appropriate category
                tools_by_category[category].append({
                    "name": row.get("Tool Name", "").strip(),
                    "url": row.get("URL", "").strip(),
                    "description": row.get("What it does", "").strip(),
                    "pricing": row.get("Free/Paid", "N/A").strip()
                })
                
    except Exception as e:
        print(f"⚠️ Error parsing master CSV: {e}")
        return {}
        
    return tools_by_category

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """
    Main function to generate the website data.
    
    This function orchestrates the complete data processing pipeline:
    1. Reads source markdown and CSV files
    2. Parses content using specialized parsing functions
    3. Combines and sorts all updates chronologically
    4. Generates TypeScript content file for the website
    5. Provides detailed logging and error handling
    
    The function implements robust error handling to ensure the website
    can still be built even if some source files are missing or malformed.
    """
    print("🚀 Starting website data preparation...")

    # Ensure artifacts directory exists
    os.makedirs(ARTIFACTS_DIR, exist_ok=True)

    # Read and parse daily tools digest
    try:
        with open(DAILY_TOOLS_MD, 'r', encoding='utf-8') as f:
            daily_tools_content = f.read()
        print("✅ Successfully read ai-tools-daily.md")
    except FileNotFoundError:
        daily_tools_content = ""
        print("🟡 ai-tools-daily.md not found, skipping daily tools.")
    except Exception as e:
        daily_tools_content = ""
        print(f"⚠️ Error reading ai-tools-daily.md: {e}")

    # Read and parse news aggregation
    try:
        with open(NEWS_MD, 'r', encoding='utf-8') as f:
            news_content = f.read()
        print("✅ Successfully read blogs-and-news.md")
    except FileNotFoundError:
        news_content = ""
        print("🟡 blogs-and-news.md not found, skipping news.")
    except Exception as e:
        news_content = ""
        print(f"⚠️ Error reading blogs-and-news.md: {e}")

    # Parse all data sources
    print("📊 Parsing content...")
    daily_tools = parse_daily_tools(daily_tools_content)
    daily_news = parse_news(news_content)
    all_tools = parse_master_tools()

    # Combine and sort all daily updates chronologically (newest first)
    all_updates = sorted(daily_tools + daily_news, key=lambda x: x['date'], reverse=True)

    # Prepare final data structure for the website
    website_data = {
        "last_updated": datetime.now().isoformat(),
        "daily_updates": all_updates,
        "tools_directory": all_tools
    }

    # Convert Python dictionary to JSON string with proper formatting
    json_string = json.dumps(website_data, indent=2, ensure_ascii=False)

    # Create TypeScript file content with proper export syntax
    ts_content = f"""// This file is auto-generated by prepare_website_data.py. Do not edit manually.
// Last generated: {datetime.now().isoformat()}

export const content = {json_string};
"""

    # Write output TypeScript file
    try:
        os.makedirs(os.path.dirname(OUTPUT_TS), exist_ok=True)
        with open(OUTPUT_TS, 'w', encoding='utf-8') as f:
            f.write(ts_content)
        
        print(f"✅ Website data successfully generated at {OUTPUT_TS}")
        print(f"   📅 {len(all_updates)} daily updates processed")
        print(f"   🛠️ {len(all_tools)} tool categories processed")
        print(f"   📊 Total tools: {sum(len(tools) for tools in all_tools.values())}")
        
    except Exception as e:
        print(f"❌ Error writing output file: {e}")
        raise

if __name__ == "__main__":
    main() 